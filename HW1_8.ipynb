{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "(*20 points extra credit) Download text from each of the websites listed below. For each data source,"
      ],
      "metadata": {
        "id": "EMhH6IwITR6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. \tlist the top most frequently occurring bigrams (HINT: use the nltk collocations() function with 30 as an input).\n",
        "b. \tSplit the text into documents\n",
        "c. \tGenerate two term-document matrices from this dataset (one where each unigram is a token, and one where bigrams can also be tokens). Import the matrix into sklearn and save it as a PKL file.  For each text file, indicate how many terms, documents, and unigram tokens are in the corpus"
      ],
      "metadata": {
        "id": "CTiGmnp8TUnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i. \tGeorge Washington’s Masonic Correspondence (UTF8) from Project Gutenberg. Treat each paragraph as one “document”\n",
        "\n",
        "ii. \tFDA Circulatory System’s Devices Panel Advisory Panel Meeting of June 23, 2005.\n",
        "Hint: use Beautiful Soup. One document is the end of a speaker’s statement\n",
        "\n",
        "iii. \tNate Silver’s Sports RSS feed http://fivethirtyeight.com/sports/feed/ One document is an article\n",
        "\n",
        "iv. \tNASA’s Systems Engineering Handbook http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20080008301.pdf -- hint: use pypdf. One document is denoted by a carriage return\n",
        "\n",
        "v. \tA sample of 10,000 tweets on a search string of your choice – hint: use tweepy or SFM. One document is a tweet. (If you are having trouble getting your own Twitter API credentials, pick a publicly available dataset that uses full JSON encoding).\n",
        "\n",
        "1. \tExtra credit: Redo your analysis on a sample of tweets that does not include retweets and tweets with URLs\n",
        "\n",
        "vi. \tDownload the text of the top 100 websites obtained by using the search string “data analytics” using a search engine API, such as Google. One document is a webpage.\n"
      ],
      "metadata": {
        "id": "7pl23PpCTVTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: this assignment will be much easier if you write a set of general functions that can take in any parsed input, instead of rewriting your code for each of i-vi."
      ],
      "metadata": {
        "id": "8hX4d64fTkSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import"
      ],
      "metadata": {
        "id": "mIbmHRf05APp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf[crypto]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptumV54jTG0",
        "outputId": "f860a96d-c95a-4f8f-aedf-92e9750636e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf[crypto]\n",
            "  Downloading pypdf-3.4.1-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from pypdf[crypto]) (4.4.0)\n",
            "Collecting PyCryptodome\n",
            "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, PyCryptodome\n",
            "Successfully installed PyCryptodome-3.17 pypdf-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcURJXyKTOzl",
        "outputId": "9d970ad9-ffda-4b5d-93ce-2eb09b4c03d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# import nlk and text classification packages\n",
        "import nltk\n",
        "import re\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "nltk.download('genesis')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import webtext\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# beautiful soup\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "HwitmH7nWx7Q"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "import requests"
      ],
      "metadata": {
        "id": "Gn_699LfjTAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# function"
      ],
      "metadata": {
        "id": "Pnf6ToLAAuhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_bigram(df_text, pattern):\n",
        "  words = df_text.lower().split()\n",
        "  biagram_collocation = BigramCollocationFinder.from_words(words)\n",
        "\n",
        "  stop_w = set(stopwords.words('english'))\n",
        "  stop_filter = lambda w: len(w) < 3 or w in stop_w\n",
        "\n",
        "  biagram_collocation.apply_word_filter(stop_filter)\n",
        "  print(f\"Word List of Bigrams: \\n {(biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 30))}\")\n",
        "\n",
        "  docs = re.split(pattern, df_text)\n",
        "  print(f\"\\n Number of sentence documents: {len(docs)}\")\n",
        "\n",
        "  vectorizer = CountVectorizer()\n",
        "  uni_matrix = vectorizer.fit_transform(docs)\n",
        "  print(f\"\\n Number of unigram : {len(vectorizer.get_feature_names_out())}\")\n",
        "\n",
        "  return (biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 30))"
      ],
      "metadata": {
        "id": "J03yQhrw9gWa"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Source with function to execute"
      ],
      "metadata": {
        "id": "BZBC9T4_5zdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gutenberg text\n",
        "r= requests.get('https://www.gutenberg.org/files/29949/29949-h/29949-h.htm')\n",
        "guten_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "guten_soup = guten_soup.getText()"
      ],
      "metadata": {
        "id": "09yRZLzk5yeg"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1 = r'\\n\\n+'\n",
        "response1 = freq_bigram(guten_soup, pattern1)"
      ],
      "metadata": {
        "id": "BdtVyUAm5yYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b667c3e-1a82-424d-eff9-ea70d5675c57"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word List of Bigrams: \n",
            " [('grand', 'lodge'), ('project', 'gutenberg-tm'), ('lodge,', 'no.'), ('grand', 'master'), ('no.', '39,'), ('alexandria', 'lodge,'), ('king', \"david's\"), ('project', 'gutenberg'), ('south', 'carolina,'), ('right', 'worshipful'), ('st.', \"john's\"), ('literary', 'archive'), ('new', 'york,'), ('gutenberg-tm', 'electronic'), ('no.', '22,'), ('united', 'states.'), ('gutenberg', 'literary'), ('prince', \"george's\"), ('united', 'states'), ('thus', 'far'), ('grand', 'master,'), ('ancient', 'york'), ('st.', 'john'), ('letter', 'book'), ('united', 'states,'), ('william', 'smith,'), ('found', 'among'), ('set', 'forth'), (\"david's\", 'lodge,'), ('mount', 'vernon')]\n",
            "\n",
            " Number of sentence documents: 189\n",
            "\n",
            " Number of unigram : 3811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(response1, 'gutenburg.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF8L6EvbpX1b",
        "outputId": "339e7a70-4dc9-467f-8bf2-f3cea9076732"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gutenburg.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('https://fivethirtyeight.com/sports/feed/')\n",
        "sport_soup = BeautifulSoup(r.text, 'html.parser')\n",
        "sport_soup = sport_soup.getText()"
      ],
      "metadata": {
        "id": "JCbnwqkShMQf"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1 = r'\\n\\n+'\n",
        "response2 = freq_bigram(sport_soup, pattern1)"
      ],
      "metadata": {
        "id": "fJQnRjkG409n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8afb41e-8631-4d81-dea3-a8dbbf2a5a0f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word List of Bigrams: \n",
            " [('</tr>', '<tr>'), ('super', 'bowl'), ('<td', 'class=\"number'), ('<tr>', '<td'), ('colspan=\"1\"', 'scope=\"col\"'), ('<th', 'colspan=\"1\"'), ('#f4f4f4;', 'border-radius:'), ('<td', 'class=\"text\"'), ('<div', 'style=\"'), ('background-color:', '#f4f4f4;'), ('style=\"', 'background-color:'), ('(max-width:', '1080px)'), ('100vw,', '(max-width:'), ('1080px)', '66vw,'), ('66vw,', '684px\"'), ('768px)', '100vw,'), ('flex;', 'flex-direction:'), ('<td', 'class=\"number\"'), ('class=\"heat', 'number\"'), ('border-radius:', '4px;'), ('border-radius:', '50%;'), ('style=\"color:', '#222222;background-color:'), ('4px;', 'flex-grow:'), ('font-family:arial,sans-serif;', 'font-size:14px;'), ('8px', 'solid'), ('solid', 'transparent;'), ('style=\"background-color:', '#f4f4f4;'), ('2px', 'solid'), ('style=\"display:', 'flex;'), ('class=\"number', 'highlight\"')]\n",
            "\n",
            " Number of sentence documents: 790\n",
            "\n",
            " Number of unigram : 6739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(response2, 'sports.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahjBqpdxpf1z",
        "outputId": "6b9b2a20-e06c-4894-8ec9-fe6e0eda28b2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sports.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    }
  ]
}